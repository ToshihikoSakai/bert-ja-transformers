{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF4_DZ3rElSU"
      },
      "outputs": [],
      "source": [
        "# å„ç¨®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³\n",
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install sentencepiece\n",
        "!pip list | grep torch\n",
        "!pip list | grep transformers\n",
        "!pip list | grep tokenizers\n",
        "!pip list | grep sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æŒ‡å®š\n",
        "dir = \"./\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8BfsEiyFFTj"
      },
      "outputs": [],
      "source": [
        "# äº‹å‰å­¦ç¿’ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ã®æº–å‚™\n",
        "# 1è¡Œã«1æ–‡ç« ã¨ãªã‚‹ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ã™ã‚‹\n",
        "\n",
        "df_header = pd.read_csv('XXX.csv')\n",
        "print(df_header)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88Vixe3_FQgY"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "from sentencepiece import SentencePieceTrainer\n",
        "\n",
        "# sentencepieceã®å­¦ç¿’\n",
        "SentencePieceTrainer.Train(\n",
        "    '--input='+dir+'corpus/corpus.txt, --model_prefix='+dir+'model/sentencepiece --character_coverage=0.9995 --vocab_size=100'\n",
        ")\n",
        "\n",
        "# sentencepieceã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "# https://github.com/google/sentencepiece#train-sentencepiece-model\n",
        "# training options\n",
        "# https://github.com/google/sentencepiece/blob/master/doc/options.md\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paV7mRK2PUiO",
        "outputId": "5d0f3459-bc98-4c83-c93f-a4822408e671"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1641: FutureWarning: Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['â–', 'å¾è¼©', 'ã¯', 'çŒ«', 'ã§ã‚ã‚‹', 'ã€‚', 'åå‰', 'ã¯', 'ã¾ã ç„¡', 'ã„', 'ã€‚']\n"
          ]
        }
      ],
      "source": [
        "# sentencepieceã®ãƒ¢ãƒ‡ãƒ«ã‚’Tokenizerã§èª­ã¿è¾¼ã¿\n",
        "\n",
        "# sentencepieceã‚’ä½¿ã£ãŸTokenizerã¯ç¾æ™‚ç‚¹ã§ã¯ä»¥ä¸‹ã€‚\n",
        "# >All transformers models in the library that use SentencePiece use it \n",
        "# in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, Marian, and T5.\n",
        "# https://huggingface.co/transformers/tokenizer_summary.html\n",
        "\n",
        "from transformers import AlbertTokenizer\n",
        "\n",
        "# ALBERTã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’å®šç¾©\n",
        "tokenizer = AlbertTokenizer.from_pretrained(dir+'model/sentencepiece.model', keep_accents=True)\n",
        "\n",
        "# textã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
        "text = \"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITNM5MyMJyNw",
        "outputId": "c2d7a698-a545-46b9-fef2-d26458bea9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No of parameters:  68158211\n"
          ]
        }
      ],
      "source": [
        "# BERTãƒ¢ãƒ‡ãƒ«ã®configã‚’è¨­å®š\n",
        "from transformers import BertConfig\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "# BERTconfigã‚’å®šç¾©\n",
        "config = BertConfig(vocab_size=32003, num_hidden_layers=12, intermediate_size=768, num_attention_heads=12)\n",
        "\n",
        "# BERT MLMã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ\n",
        "model = BertForMaskedLM(config)\n",
        "\n",
        "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º\n",
        "print('No of parameters: ', model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2oLQ9mWQ0bQ",
        "outputId": "4d288489-8f61-46d7-b3ba-752163919661"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# äº‹å‰å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# textã‚’1è¡Œãšã¤èª­ã¿è¾¼ã‚“ã§ãƒˆãƒ¼ã‚¯ãƒ³ã¸å¤‰æ›\n",
        "dataset = LineByLineTextDataset(\n",
        "     tokenizer=tokenizer,\n",
        "     file_path=dir + 'corpus/corpus.txt',\n",
        "     block_size=256, # tokenizerã®max_length\n",
        ")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãã‚Œã‚‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã®è¾æ›¸ã¨ã—ã¦ãƒãƒƒãƒã«ç…§åˆã™ã‚‹ãŸã‚ã®é–¢æ•°\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, \n",
        "    mlm=True,\n",
        "    mlm_probability= 0.15\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "uo9-0Me9WfgD",
        "outputId": "b5a60954-cc8c-4f07-8b93-6142ce7ea1e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running training *****\n",
            "  Num examples = 5\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:00, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/\n",
            "Configuration saved in /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# äº‹å‰å­¦ç¿’ã‚’è¡Œã†\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "# äº‹å‰å­¦ç¿’ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= drive_dir + 'outputBERT/',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True\n",
        ")\n",
        "\n",
        "# trainerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç”Ÿæˆ\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "#ã€€å­¦ç¿’\n",
        "trainer.train()\n",
        "\n",
        "# å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
        "trainer.save_model(dir + 'outputBERT/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG6wHOLlXDjj",
        "outputId": "0a8e4cfb-780c-42c7-bd1f-caaf81b903c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1641: FutureWarning: Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "loading file /content/drive/MyDrive/BERT-pretrained-transformers/model/sentencepiece.model\n",
            "Adding [CLS] to the vocabulary\n",
            "Adding [SEP] to the vocabulary\n",
            "Adding <pad> to the vocabulary\n",
            "Adding [MASK] to the vocabulary\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 768,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32003\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
            "\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.0019151709275320172,\n",
              "  'sequence': 'ã¯ã€ ã§ã‚ã‚‹ã€‚ã¯ã„ã€‚',\n",
              "  'token': 3,\n",
              "  'token_str': 'ã€'},\n",
              " {'score': 0.0012296646600589156,\n",
              "  'sequence': 'ã¯ã‹ ã§ã‚ã‚‹ã€‚ã¯ã„ã€‚',\n",
              "  'token': 9,\n",
              "  'token_str': 'ã‹'},\n",
              " {'score': 0.0007844513165764511,\n",
              "  'sequence': 'ã¯ã€‚ ã§ã‚ã‚‹ã€‚ã¯ã„ã€‚',\n",
              "  'token': 7,\n",
              "  'token_str': 'ã€‚'},\n",
              " {'score': 0.0006089677917771041,\n",
              "  'sequence': 'ã¯ã¨ ã§ã‚ã‚‹ã€‚ã¯ã„ã€‚',\n",
              "  'token': 6,\n",
              "  'token_str': 'ã¨'},\n",
              " {'score': 0.0005491935880854726,\n",
              "  'sequence': 'ã¯ã® ã§ã‚ã‚‹ã€‚ã¯ã„ã€‚',\n",
              "  'token': 8,\n",
              "  'token_str': 'ã®'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª\n",
        "from transformers import pipeline\n",
        "\n",
        "# tokenizerã¨model\n",
        "tokenizer = AlbertTokenizer.from_pretrained(drive_dir+'model/sentencepiece.model', keep_accents=True)\n",
        "model = BertForMaskedLM.from_pretrained(drive_dir + 'outputBERT')\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "MASK_TOKEN = tokenizer.mask_token\n",
        "\n",
        "# ã‚³ãƒ¼ãƒ‘ã‚¹ã«å¿œã˜ãŸæ–‡ç« ã‹ã‚‰ç©´åŸ‹ã‚ã‚’ã¨ã\n",
        "\n",
        "text = \"XXX{}XXX\".format(MASK_TOKEN)\n",
        "fill_mask(text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT-pretrained-transformers.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}