{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF4_DZ3rElSU"
      },
      "outputs": [],
      "source": [
        "# 各種パッケージのインストールとバージョン\n",
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install sentencepiece\n",
        "!pip list | grep torch\n",
        "!pip list | grep transformers\n",
        "!pip list | grep tokenizers\n",
        "!pip list | grep sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ディレクトリの指定\n",
        "dir = \"./\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8BfsEiyFFTj"
      },
      "outputs": [],
      "source": [
        "# 事前学習用コーパスの準備\n",
        "# 1行に1文章となるようなテキストを準備する\n",
        "\n",
        "df_header = pd.read_csv('XXX.csv')\n",
        "print(df_header)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88Vixe3_FQgY"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "from sentencepiece import SentencePieceTrainer\n",
        "\n",
        "# sentencepieceの学習\n",
        "SentencePieceTrainer.Train(\n",
        "    '--input='+dir+'corpus/corpus.txt, --model_prefix='+dir+'model/sentencepiece --character_coverage=0.9995 --vocab_size=100'\n",
        ")\n",
        "\n",
        "# sentencepieceのパラメータ\n",
        "# https://github.com/google/sentencepiece#train-sentencepiece-model\n",
        "# training options\n",
        "# https://github.com/google/sentencepiece/blob/master/doc/options.md\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paV7mRK2PUiO",
        "outputId": "5d0f3459-bc98-4c83-c93f-a4822408e671"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1641: FutureWarning: Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', '吾輩', 'は', '猫', 'である', '。', '名前', 'は', 'まだ無', 'い', '。']\n"
          ]
        }
      ],
      "source": [
        "# sentencepieceのモデルをTokenizerで読み込み\n",
        "\n",
        "# sentencepieceを使ったTokenizerは現時点では以下。\n",
        "# >All transformers models in the library that use SentencePiece use it \n",
        "# in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, Marian, and T5.\n",
        "# https://huggingface.co/transformers/tokenizer_summary.html\n",
        "\n",
        "from transformers import AlbertTokenizer\n",
        "\n",
        "# ALBERTのトークナイザを定義\n",
        "tokenizer = AlbertTokenizer.from_pretrained(dir+'model/sentencepiece.model', keep_accents=True)\n",
        "\n",
        "# textをトークナイズ\n",
        "text = \"吾輩は猫である。名前はまだ無い。\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITNM5MyMJyNw",
        "outputId": "c2d7a698-a545-46b9-fef2-d26458bea9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No of parameters:  68158211\n"
          ]
        }
      ],
      "source": [
        "# BERTモデルのconfigを設定\n",
        "from transformers import BertConfig\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "# BERTconfigを定義\n",
        "config = BertConfig(vocab_size=32003, num_hidden_layers=12, intermediate_size=768, num_attention_heads=12)\n",
        "\n",
        "# BERT MLMのインスタンスを生成\n",
        "model = BertForMaskedLM(config)\n",
        "\n",
        "# パラメータ数を表示\n",
        "print('No of parameters: ', model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2oLQ9mWQ0bQ",
        "outputId": "4d288489-8f61-46d7-b3ba-752163919661"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# 事前学習用のデータセットを準備\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# textを1行ずつ読み込んでトークンへ変換\n",
        "dataset = LineByLineTextDataset(\n",
        "     tokenizer=tokenizer,\n",
        "     file_path=dir + 'corpus/corpus.txt',\n",
        "     block_size=256, # tokenizerのmax_length\n",
        ")\n",
        "\n",
        "# データセットからサンプルのリストを受け取り、それらをテンソルの辞書としてバッチに照合するための関数\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, \n",
        "    mlm=True,\n",
        "    mlm_probability= 0.15\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "uo9-0Me9WfgD",
        "outputId": "b5a60954-cc8c-4f07-8b93-6142ce7ea1e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running training *****\n",
            "  Num examples = 5\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:00, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/\n",
            "Configuration saved in /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# 事前学習を行う\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "# 事前学習のパラメータを定義\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= drive_dir + 'outputBERT/',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True\n",
        ")\n",
        "\n",
        "# trainerインスタンスの生成\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "#　学習\n",
        "trainer.train()\n",
        "\n",
        "# 学習したモデルの保存\n",
        "trainer.save_model(dir + 'outputBERT/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG6wHOLlXDjj",
        "outputId": "0a8e4cfb-780c-42c7-bd1f-caaf81b903c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1641: FutureWarning: Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "loading file /content/drive/MyDrive/BERT-pretrained-transformers/model/sentencepiece.model\n",
            "Adding [CLS] to the vocabulary\n",
            "Adding [SEP] to the vocabulary\n",
            "Adding <pad> to the vocabulary\n",
            "Adding [MASK] to the vocabulary\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 768,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32003\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
            "\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/BERT-pretrained-transformers/outputBERT.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.0019151709275320172,\n",
              "  'sequence': 'は、 である。はい。',\n",
              "  'token': 3,\n",
              "  'token_str': '、'},\n",
              " {'score': 0.0012296646600589156,\n",
              "  'sequence': 'はか である。はい。',\n",
              "  'token': 9,\n",
              "  'token_str': 'か'},\n",
              " {'score': 0.0007844513165764511,\n",
              "  'sequence': 'は。 である。はい。',\n",
              "  'token': 7,\n",
              "  'token_str': '。'},\n",
              " {'score': 0.0006089677917771041,\n",
              "  'sequence': 'はと である。はい。',\n",
              "  'token': 6,\n",
              "  'token_str': 'と'},\n",
              " {'score': 0.0005491935880854726,\n",
              "  'sequence': 'はの である。はい。',\n",
              "  'token': 8,\n",
              "  'token_str': 'の'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 言語モデルの確認\n",
        "from transformers import pipeline\n",
        "\n",
        "# tokenizerとmodel\n",
        "tokenizer = AlbertTokenizer.from_pretrained(drive_dir+'model/sentencepiece.model', keep_accents=True)\n",
        "model = BertForMaskedLM.from_pretrained(drive_dir + 'outputBERT')\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "MASK_TOKEN = tokenizer.mask_token\n",
        "\n",
        "# コーパスに応じた文章から穴埋めをとく\n",
        "\n",
        "text = \"XXX{}XXX\".format(MASK_TOKEN)\n",
        "fill_mask(text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT-pretrained-transformers.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}